{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../../project\"))\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "from analysis.parser import clean, get_resume_sections\n",
    "\n",
    "df = pd.read_csv(\"UpdatedResumeDataSet.csv\", encoding=\"utf-8\")\n",
    "df.drop_duplicates(subset=[\"Resume\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"Clean\"] = df[\"Resume\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = []\n",
    "for row in df[\"Clean\"]:\n",
    "    lengths.append(len(row))\n",
    "\n",
    "plt.plot(lengths)\n",
    "plt.title(\"length of resumes\")\n",
    "plt.ylabel(\"number of chars\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"thenlper/gte-base\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this, reduce chunk_size and chunk_overlap from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello there my guy you are such a guy dude guy yaya yaya\"\n",
    "\n",
    "def chunk_data(text):\n",
    "    docs = splitter.create_documents(texts=[text])\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    return embedding_model.embed_documents(chunks)\n",
    "\n",
    "chunks = chunk_data(text)\n",
    "print(chunks)\n",
    "embedded = embed_chunks(chunks)\n",
    "print(embedded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "df[\"Chunks\"] = df[\"Clean\"].apply(chunk_data)\n",
    "\n",
    "def embed_chunks_map(sample):\n",
    "    return {\n",
    "        \"Embeddings\" : embed_chunks(sample[\"Chunks\"])\n",
    "    }\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "start = time()\n",
    "ds = ds.map(embed_chunks_map)\n",
    "print(time() - start)\n",
    "\n",
    "df = ds.to_pandas()\n",
    "print(time() - start)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding lengths are determined by model, so all will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df[\"Embeddings\"][0]\n",
    "lengths = set([len(x) for x in example])\n",
    "print(lengths)\n",
    "EMBEDDING_LENGTH = list(lengths)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download postgres and run service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statements to understand\n",
    "```sql\n",
    "CREATE TABLE name(\n",
    "    val1 type1,\n",
    "    val2 type2,\n",
    "    val3 type3\n",
    ")   \n",
    "```\n",
    "Create a table called name where each row (or record) keeps information on keys val1... with type type1...\n",
    "```sql\n",
    "INSERT INTO name (val1, val2, val3) VALUES (input1, input2, input3)\n",
    "```\n",
    "Insert the new record where val1 = input1, etc\n",
    "```sql\n",
    "SELECT * FROM name\n",
    "```\n",
    "Get every column of data for every row of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pgvector(vec):\n",
    "    return f\"[{', '.join(map(str, vec))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "\n",
    "TABLE_NAME = \"resume_docs\"\n",
    "\n",
    "def load_db(df):\n",
    "    with psycopg.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",  # Use localhost instead of a UNIX socket\n",
    "        port=\"5432\"\n",
    "    ) as conn:\n",
    "        # conn.autocommit = True\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "            register_vector(conn)\n",
    "\n",
    "            cur.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "            cur.execute(\n",
    "                f\"\"\"\n",
    "                CREATE TABLE {TABLE_NAME}(\n",
    "                    id serial PRIMARY KEY,\n",
    "                    source_id integer,\n",
    "                    chunk text,\n",
    "                    embedding vector({EMBEDDING_LENGTH})\n",
    "                )   \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            for i, row in df.iterrows():\n",
    "                embeds = row[\"Embeddings\"]\n",
    "                chunks = row[\"Chunks\"]\n",
    "                for j in range(len(embeds)):\n",
    "                    vector_str = to_pgvector(embeds[j])\n",
    "                    cur.execute(\n",
    "                        f\"INSERT INTO {TABLE_NAME} (source_id, chunk, embedding) VALUES (%s, %s, %s)\", \n",
    "                        (i, chunks[j], vector_str)\n",
    "                    )\n",
    "\n",
    "            for x in cur.execute(f\"SELECT id, source_id FROM {TABLE_NAME}\"):\n",
    "                print(x)\n",
    "\n",
    "load_db(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting top K for SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General form:\n",
    "```sql\n",
    "SELECT * FROM table_name ORDER BY expression LIMIT k\n",
    "```\n",
    "Sort table_name by some some expression eg. (arg1, |arg1 - input|). Take just the top K\n",
    "\n",
    "\n",
    "For our use case, it will look like this:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM resume_docs ORDER BY embedding <-> query::vector LIMIT k\n",
    "```\n",
    "Where we order by embedding <-> query::vector where <-> gets cosine similarity from each rows embedding to our query embedding, which we explicitly cast to vector with ::vector. Then we take k closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we embed our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me someone with software engineering experience\"\n",
    "query = embedding_model.embed_query(query)\n",
    "print(query)\n",
    "print(len(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(prompt, k):\n",
    "    embedded_prompt = embedding_model.embed_query(prompt)\n",
    "    prompt_vec = to_pgvector(embedded_prompt)\n",
    "\n",
    "    with psycopg.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            res = cur.execute(\n",
    "                f\"\"\"\n",
    "                SELECT *, embedding <-> %s::vector AS similarity\n",
    "                FROM {TABLE_NAME}\n",
    "                ORDER BY similarity\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (prompt_vec, k)\n",
    "            ).fetchall()\n",
    "            return [\n",
    "                {\n",
    "                    \"score\": record[4],\n",
    "                    \"source_id\": record[1]\n",
    "                }\n",
    "                for record in res\n",
    "            ]\n",
    "\n",
    "query = \"experience with machine learning and data analytics\"\n",
    "k = 10\n",
    "res = get_top_k(query, k)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# these \n",
    "def load_resumes(ids: List[int]):\n",
    "    ids = list(set(ids))\n",
    "    data = \"\"\n",
    "    for id in ids:\n",
    "        add = f\"### Applicant ID {id}\\n{df['Clean'][id]}\\n\\n\"\n",
    "        data += add\n",
    "    return data\n",
    "\n",
    "ids = [vals[\"source_id\"] for vals in res]\n",
    "print(load_resumes(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling openai sdk which deepseek and other non open ai models also use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"write a haiku about ai\"\n",
    "        }, # task or main prompt\n",
    "        {\n",
    "            \"role\": \"developer\", \n",
    "            \"content\": \"\"\"You are a helpful assistant\n",
    "                        that answers programming\n",
    "                        questions in the style of a\n",
    "                        southern belle from the\n",
    "                        southeast United States.\"\"\"\n",
    "        }, # personality \n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"\"\"Bits dance in the night,\n",
    "                        logic weaves through lines of code,\n",
    "                        dreams compile to life.\"\"\"\n",
    "        }, # reference responses \n",
    "    ],\n",
    "    temperature=0, # how creative where 0 is deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion)\n",
    "print(completion.choices)\n",
    "print(completion.choices[0])\n",
    "print(completion.choices[0].message)\n",
    "\n",
    "print()\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query(prompt, k=10):\n",
    "    top_k = get_top_k(prompt, k)\n",
    "    ids = [vals[\"source_id\"] for vals in top_k]\n",
    "    print(ids)\n",
    "    context = load_resumes(ids)\n",
    "    developer_prompt = \"You are an expert in talent acquisition and tasked with analyzing and comparing a set resumes to select the best applicants for hire\"\n",
    "    query_prompt = f\"\"\"## Context:\\n{context}\\n\\n## Query: {prompt}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\":  query_prompt\n",
    "            }, # task or main prompt\n",
    "            {\n",
    "                \"role\": \"developer\", \n",
    "                \"content\": developer_prompt\n",
    "            }, # personality \n",
    "        ],\n",
    "        temperature=0, # how creative where 0 is deterministic\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "prompt = \"give me applicants with the most web development experience and explain why they would be fit for fast development\"\n",
    "res = query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
