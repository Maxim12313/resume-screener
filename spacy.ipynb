{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufXD7bLmSMWt"
      },
      "source": [
        "# Resume Parsing with SpaCy\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we will explore how to use **SpaCy**, a powerful NLP library in Python, to parse resumes. We'll focus on extracting key information like names, phone numbers, emails, and LinkedIn URLs.\n",
        "\n",
        "### What is SpaCy?\n",
        "SpaCy is an open-source library for advanced **Natural Language Processing (NLP)** in Python. It is designed specifically for production use and offers:\n",
        "- **Pre-trained Models**: SpaCy provides models trained on large datasets to identify entities like names, organizations, and dates.\n",
        "- **Efficiency and Speed**: Built with performance in mind, SpaCy is one of the fastest NLP libraries.\n",
        "- **Customizable Pipelines**: You can easily add or modify components like tokenizers, taggers, and entity recognizers to fit your specific needs.\n",
        "- **Wide Applications**: From text classification and sentiment analysis to information extraction and resume parsing, SpaCy is versatile and powerful.\n",
        "\n",
        "### Why Use SpaCy for Resume Parsing?\n",
        "- **Pre-trained NER models** for entity extraction\n",
        "- **Interactive and customizable** for specialized tasks like resume parsing\n",
        "- **Fast and efficient** processing for large datasets\n",
        "- **Easy integration** with other Python libraries and tools\n",
        "\n",
        "#### Helpful Resources:\n",
        "- [SpaCy Documentation](https://spacy.io/usage)\n",
        "- [Named Entity Recognition in SpaCy](https://spacy.io/usage/linguistic-features#named-entities)\n",
        "- [Custom Components in SpaCy Pipelines](https://spacy.io/usage/processing-pipelines)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvOGEg6vSMWw"
      },
      "outputs": [],
      "source": [
        "# Install SpaCy if you haven't already\n",
        "!pip install pdfplumber\n",
        "!pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNeiAiKXSMWy"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import re\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt5VdasXSMWz"
      },
      "source": [
        "## Step 1: Load SpaCy Model\n",
        "We'll start by loading the small English model `en_core_web_sm`, which includes tokenization, POS tagging, and NER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNzClw9fSMW0"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6AbDgeZSMW0"
      },
      "source": [
        "## Step 2: Extract Text from a Resume PDF\n",
        "Use `pdfplumber` to extract text from the resume PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7KUXAXpSMW1"
      },
      "outputs": [],
      "source": [
        "with pdfplumber.open(\"examples/jakes-resume.pdf\") as pdf:\n",
        "    resume_text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "\n",
        "print(resume_text[:500])  # Displaying the first 500 characters of the resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqTICvjSMW2"
      },
      "source": [
        "## Step 3: Named Entity Recognition (NER)\n",
        "SpaCy's NER can recognize entities like names, organizations, and more. Let's see what it can extract from the resume.\n",
        "\n",
        "### Exercise:\n",
        "After running the code, try highlighting specific entities (like `ORG`, `PERSON`, or `GPE`) to see how well SpaCy detects them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32YjK6VBSMW2"
      },
      "outputs": [],
      "source": [
        "doc = nlp(resume_text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.label_}: {ent.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P39GjhhSMW3"
      },
      "source": [
        "## Step 4: Custom Pattern Matching with SpaCy's Matcher\n",
        "For entities like phone numbers, emails, and LinkedIn URLs, we can create custom patterns using `Matcher`.\n",
        "\n",
        "### Example Patterns:\n",
        "- **Phone Number**: Sequence of digits and optional symbols like `(`, `)`, `-`, and spaces.\n",
        "- **Email**: Text patterns with `@` symbol.\n",
        "- **LinkedIn URL**: URLs containing \"linkedin.com/in/\".\n",
        "\n",
        "### Exercise:\n",
        "Try tweaking the patterns to see if you can improve the detection of phone numbers or LinkedIn URLs. (if they don't work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhOg3Q5RSMW3"
      },
      "outputs": [],
      "source": [
        "# Regex pattern for phone numbers\n",
        "phone_regex = r'(\\+\\d{1,2}\\s)?(\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})'\n",
        "phones = re.findall(phone_regex, resume_text)\n",
        "\n",
        "# Regex pattern for LinkedIn URLs\n",
        "linkedin_regex = r'linkedin\\.com/(in|pub)/[A-Za-z0-9-_/]+'\n",
        "linkedins = re.findall(linkedin_regex, resume_text)\n",
        "\n",
        "# Extract emails using SpaCy Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "email_pattern = [\n",
        "    {\"TEXT\": {\"REGEX\": r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\"}}\n",
        "]\n",
        "matcher.add(\"EMAIL\", [email_pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "emails = []\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    emails.append(span.text)\n",
        "\n",
        "print(f\"Extracted Phones: {[phone[1] for phone in phones]}\")\n",
        "print(f\"Extracted Emails: {emails}\")\n",
        "print(f\"Extracted LinkedIn URLs: {[f'https://linkedin.com/{linkedin}' for linkedin in linkedins]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUnhliHVSMW3"
      },
      "source": [
        "## Step 5: Extracting Name Using NER\n",
        "SpaCy's NER can often detect the candidate's name under the `PERSON` label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xKtVVq9SMW3"
      },
      "outputs": [],
      "source": [
        "name = \"\"\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"PERSON\":\n",
        "        name = ent.text\n",
        "        break\n",
        "\n",
        "print(f\"Extracted Name: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3R01vQoSMW4"
      },
      "source": [
        "## Step 6: Your Turn - Extract Other Entities!\n",
        "Now it's your turn to extract other entities. Try finding:\n",
        "1. **Organizations (ORG)**\n",
        "2. **Locations (GPE)**\n",
        "3. **Dates (DATE)**\n",
        "\n",
        "### Exercise:\n",
        "Modify the code below to extract these entities and see how SpaCy performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oOBxbz_SMW4"
      },
      "outputs": [],
      "source": [
        "# Example for extracting organizations\n",
        "organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "print(f\"Extracted Organizations: {organizations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMS78d_SMW4"
      },
      "source": [
        "## Step 7: Combining All Extracted Information\n",
        "We'll now compile all extracted details into a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rK7e5qTSMW5"
      },
      "outputs": [],
      "source": [
        "parsed_resume = {\n",
        "    \"Name\": name,\n",
        "    \"Phone\": phones[0][1],\n",
        "    \"Email\": emails[0],\n",
        "    \"LinkedIn\": f'https://linkedin.com/{linkedins[0]}'\n",
        "}\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    match_label = nlp.vocab.strings[match_id]\n",
        "    if match_label == \"EMAIL\":\n",
        "        parsed_resume[\"Email\"] = span.text\n",
        "\n",
        "print(parsed_resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-yrDSnSMW5"
      },
      "source": [
        "## Final Thoughts\n",
        "- **SpaCy** offers powerful tools for both general and custom text extraction.\n",
        "- You can further enhance this by training a **custom NER model** for more specialized resume parsing.\n",
        "- Integrate this into larger applications to automate resume screening.\n",
        "\n",
        "Happy Parsing! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}